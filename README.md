# python-competence-analysis
# Evaluating Open Source Models for Student Competence Analysis

## Project Overview
This repository contains my submission for Python Screening Task 3. The goal is to research and evaluate open-source AI models for analyzing student-written Python code to generate meaningful prompts that assess conceptual understanding.

## Contents
- `research_plan.md`: The research plan and detailed reasoning as required by the task.
- `README.md`: This file.

## Research Summary
The plan involves identifying suitable models (like CodeT5+) through a structured landscape analysis, evaluating them based on specific criteria (code understanding, pedagogical prompt generation, computational cost), and validating their effectiveness through a mixed-methods approach combining automated metrics and human educator reviews.

## Model Choice
The initial model chosen for evaluation is **CodeT5+**, due to its strong performance on code comprehension tasks and balance between capability and size.

## Setup Instructions (Conceptual)
This repository contains a research plan. A full implementation would require:
1.  Setting up a Python environment with deep learning libraries (PyTorch/TensorFlow, Transformers).
2.  Access to the Hugging Face `transformers` library to load models like CodeT5+.
3.  A dataset of student Python code samples for evaluation.

## Contact
[Your Name or GitHub Username]
